{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b00ed94",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this notebook we will explore the basic use-case of training and evaluating `franken` on a small dataset of DFT calculations.\n",
    "\n",
    "We use the H2O data obtained from DFT calculations (using RPBE+D3 theory), originally collected by [Montero de Hijes et al.](https://doi.org/10.1063/5.0197105).\n",
    "\n",
    "To showcase the sample efficiency of our model - and to fit within a small computational budget - we finetune the [MACE-MP0 foundation model](https://mace-docs.readthedocs.io/en/latest/guide/foundation_models.html) with only 8 new samples.\n",
    "\n",
    "Note that while the original, zero-shot MACE model has poor accuracy on this dataset (which is out-of-distribution with respect to the foundation model's training data), the finetuned `franken` model is very accurate at predicting energy and forces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68183f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import ase.io\n",
    "\n",
    "import franken.metrics\n",
    "from franken.datasets.registry import DATASET_REGISTRY\n",
    "from franken.trainers.rf_cuda_lowmem import RandomFeaturesTrainer\n",
    "from franken.data.base import BaseAtomsDataset\n",
    "from franken.backbones.utils import CacheDir\n",
    "from franken.rf.model import FrankenPotential\n",
    "from franken.data.base import Target\n",
    "from franken.config import MaceBackboneConfig, MultiscaleGaussianRFConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5e5c1",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Here we load 8 random samples from the training set, and the validation set which contains 100 structures.\n",
    "\n",
    "The dataset will be downloaded into franken's cache directory (`CacheDir.get()`).\n",
    "\n",
    "\n",
    "> NOTE: The **cache directory** is used to store downloaded datasets and model backbones.\n",
    "> it defaults to `$HOME/.franken` for the current user and it can be configured by setting the `$FRANKEN_CACHE_DIR`\n",
    "> environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c8b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to configure the GNN backbone we wish to use\n",
    "# since the data format depends on which backbone will be loaded.\n",
    "# We will use the MACE-L0 backbone, with features extracted at the 2nd layer\n",
    "gnn_config = MaceBackboneConfig(\n",
    "    path_or_id=\"MACE-L0\",\n",
    "    interaction_block=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653d26dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7686f11ca77d4f498e7fb67f562f3038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (train):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dset_8 = BaseAtomsDataset.from_path(\n",
    "    data_path=DATASET_REGISTRY.get_path(\"water\", \"train\", base_path=CacheDir.get()),\n",
    "    split=\"train\",\n",
    "    num_random_subsamples=8,\n",
    "    subsample_rng=42,\n",
    "    gnn_config=gnn_config,\n",
    ")\n",
    "train_dl_8 = train_dset_8.get_dataloader(distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3380ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71528f341e4407b18dccac191f7fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (val):   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dset = BaseAtomsDataset.from_path(\n",
    "    data_path=DATASET_REGISTRY.get_path(\"water\", \"val\", base_path=CacheDir.get()),\n",
    "    split=\"val\",\n",
    "    gnn_config=gnn_config,\n",
    ")\n",
    "val_dl = val_dset.get_dataloader(distributed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8a32a",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "We start in the most explicit setting where all the hyperparameters are set before-hand. Below we describe them one by one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925f5f2",
   "metadata": {},
   "source": [
    "First we define **the kernel**, and its hyperparameters.\n",
    "Here we use the multiscale Gaussian kernel, which can fit a range of length-scales $\\sigma$ with a single model.\n",
    "This has the big advantage of requiring little tuning: a range of sensible values of $\\sigma$ can be set through the three parameters `\"length_scale_low\", \"length_scale_high\", \"length_scale_num\"`. This is much easier than precisely setting the optimal length0scale, and retains most of the accuracy.\n",
    "\n",
    "The other parameter is the number of random features, which can generally be set based on how much computing time and memory is available: more random features increase accuracy monotonically, although diminishing returns kick in after $\\approx 16000$ random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9ee68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_config = MultiscaleGaussianRFConfig(\n",
    "    num_random_features=512,\n",
    "    length_scale_low=4.0,\n",
    "    length_scale_high=24.0,\n",
    "    length_scale_num=4,\n",
    "    rng_seed=42,  # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f451a7e",
   "metadata": {},
   "source": [
    "The **linear system hyperparameters** are:\n",
    " - The L2 regularization weight (`\"l2_penalty\"`), which should be a small number and can be increased if numerical issues arise.\n",
    " - The weight of the forces compared to energies in the overall loss. If `\"force_weight\"` is closer to 1, the forces have more weight -- which is typically the desired configuration -- while when it is set closer to 0, the energies have more weight.\n",
    "\n",
    " It is generally very easy and fast to do a hyperparameter search on these parameters, but for the sake of simplicity here they are defined explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5b7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_params = {\n",
    "    \"l2_penalty\": [1e-5],\n",
    "    \"force_weight\": [0.8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca189a1",
   "metadata": {},
   "source": [
    "Instantiate classes for the model and for the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e97022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FrankenPotential(\n",
    "    gnn_config=gnn_config,\n",
    "    rf_config=rf_config,\n",
    "    num_species=2,        # H and O\n",
    "    jac_chunk_size=12,    # chosen to fit in the T4 GPU on colab. You can set it to \"auto\" to adapt to available GPU memory.\n",
    ")\n",
    "trainer = RandomFeaturesTrainer(\n",
    "    train_dataloader=train_dl_8,\n",
    "    save_every_model=False,\n",
    "    device=device,\n",
    "    save_fmaps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b2dcb",
   "metadata": {},
   "source": [
    "Call the `fit` method to **train the model**. A separate model for all parameter possibilities in the `solver_params` dictionary is trained, but note that in this case a single possibility has been specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a340721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d70e66e66949a49bede3d42e3d810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing dataset statistics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    covs+coeffs      | -0.0 cfgs/s |   0% | 1 x  13th Gen Intel(R) Core(TM) i5-1335U\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logs, weights \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/trainers/rf_cuda_lowmem.py:141\u001b[0m, in \u001b[0;36mRandomFeaturesTrainer.fit\u001b[0;34m(self, model, solver_params)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_hash \u001b[38;5;241m=\u001b[39m model_hash\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    140\u001b[0m t_cov_coeffs_start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_covs_and_coeffs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m t_cov_coeffs \u001b[38;5;241m=\u001b[39m perf_counter() \u001b[38;5;241m-\u001b[39m t_cov_coeffs_start\n\u001b[1;32m    144\u001b[0m solver_grid_size \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod([\u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m solver_params\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/trainers/rf_cuda_lowmem.py:389\u001b[0m, in \u001b[0;36mRandomFeaturesTrainer._compute_covs_and_coeffs\u001b[0;34m(self, model, dataloader)\u001b[0m\n\u001b[1;32m    386\u001b[0m energy_per_atom \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39menergy \u001b[38;5;241m/\u001b[39m data\u001b[38;5;241m.\u001b[39mnatoms\n\u001b[1;32m    387\u001b[0m forces_per_atom \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mforces \u001b[38;5;241m/\u001b[39m data\u001b[38;5;241m.\u001b[39mnatoms\n\u001b[0;32m--> 389\u001b[0m forces_fmap, energy_fmap \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_feature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m energy_fmap \u001b[38;5;241m=\u001b[39m energy_fmap\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_dt)\n\u001b[1;32m    392\u001b[0m rank1_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiag_energy, energy_fmap, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/rf/model.py:202\u001b[0m, in \u001b[0;36mFrankenPotential.grad_feature_map\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    193\u001b[0m     jac_chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_jacobian_chunk_size(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_map_aux, [data\u001b[38;5;241m.\u001b[39matom_pos, data], argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grad_fmap_jacfn \u001b[38;5;241m=\u001b[39m jacfwd(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_map_aux,\n\u001b[1;32m    198\u001b[0m         argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    199\u001b[0m         has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mjac_chunk_size,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grad_fmap_jacfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matom_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/utils/jac.py:48\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     45\u001b[0m     _, jvp_out \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[0;32m---> 48\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m     50\u001b[0m     results, aux \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/_functorch/vmap.py:320\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     chunks_flat_args \u001b[38;5;241m=\u001b[39m _get_chunked_inputs(\n\u001b[1;32m    318\u001b[0m         flat_args, flat_in_dims, batch_size, chunk_size\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chunked_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    332\u001b[0m     func,\n\u001b[1;32m    333\u001b[0m     batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/_functorch/vmap.py:430\u001b[0m, in \u001b[0;36m_chunked_vmap\u001b[0;34m(func, flat_in_dims, chunks_flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_rng_state(rs)\n\u001b[1;32m    429\u001b[0m     chunks_output\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 430\u001b[0m         \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    442\u001b[0m flat_output_chunks, arg_spec \u001b[38;5;241m=\u001b[39m _flatten_chunks_output(chunks_output)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# chunked output tensors are held by both `flat_output_chunks` and `chunks_output`.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# eagerly remove the reference from `chunks_output`.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/utils/jac.py:37\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_jvp\u001b[39m(basis):\n\u001b[0;32m---> 37\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;241m0\u001b[39m], is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:1139\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m   1138\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1139\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/rf/model.py:178\u001b[0m, in \u001b[0;36mFrankenPotential._feature_map_aux\u001b[0;34m(self, atom_pos, data)\u001b[0m\n\u001b[1;32m    176\u001b[0m old_atom_pos \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39matom_pos\n\u001b[1;32m    177\u001b[0m data\u001b[38;5;241m.\u001b[39matom_pos \u001b[38;5;241m=\u001b[39m atom_pos\n\u001b[0;32m--> 178\u001b[0m random_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m data\u001b[38;5;241m.\u001b[39matom_pos \u001b[38;5;241m=\u001b[39m old_atom_pos\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m random_features, random_features\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/rf/model.py:164\u001b[0m, in \u001b[0;36mFrankenPotential.feature_map\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Configuration):\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Obtain an embedding of each atom, and map it through\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    random features. The RF mapping computes an average so\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    the final feature map is per-configuration, instead of\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    per-atom.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     gnn_descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescriptors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     normalized_descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_scaler(\n\u001b[1;32m    167\u001b[0m         gnn_descriptors,\n\u001b[1;32m    168\u001b[0m         atomic_numbers\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39matomic_numbers,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrf\u001b[38;5;241m.\u001b[39mfeature_map(\n\u001b[1;32m    171\u001b[0m         normalized_descriptors,\n\u001b[1;32m    172\u001b[0m         atomic_numbers\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39matomic_numbers,\n\u001b[1;32m    173\u001b[0m     )\n",
      "File \u001b[0;32m~/Dropbox/unige/MolecularDynamics/franken/franken/backbones/wrappers/mace_wrap.py:56\u001b[0m, in \u001b[0;36mFrankenMACE.descriptors\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     54\u001b[0m node_feats_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interaction, product \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteractions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducts):\n\u001b[0;32m---> 56\u001b[0m     node_feats, sc \u001b[38;5;241m=\u001b[39m \u001b[43minteraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m product(node_feats\u001b[38;5;241m=\u001b[39mnode_feats, sc\u001b[38;5;241m=\u001b[39msc, node_attrs\u001b[38;5;241m=\u001b[39mnode_attrs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Extract only scalars. Use `irreps_out` attribute to figure out which features correspond to scalars.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# irreps_out is an `Irreps` object: a 2-tuple of multiplier and `Irrep` objects.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Tuple[Tuple[int, Tuple[int, int]], Tuple[int, Tuple[int, int]]]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Note this is equivalent code, which does not support TorchScript.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# invariant_slices = product.linear.irreps_out.slices()[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/mace/modules/blocks.py:750\u001b[0m, in \u001b[0;36mRealAgnosticResidualInteractionBlock.forward\u001b[0;34m(self, node_attrs, node_feats, edge_attrs, edge_feats, edge_index)\u001b[0m\n\u001b[1;32m    748\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_tp(node_feats, node_attrs)\n\u001b[1;32m    749\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_up(node_feats)\n\u001b[0;32m--> 750\u001b[0m tp_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_tp_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m mji \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_tp(\n\u001b[1;32m    752\u001b[0m     node_feats[sender], edge_attrs, tp_weights\n\u001b[1;32m    753\u001b[0m )  \u001b[38;5;66;03m# [n_edges, irreps]\u001b[39;00m\n\u001b[1;32m    754\u001b[0m message \u001b[38;5;241m=\u001b[39m scatter_sum(\n\u001b[1;32m    755\u001b[0m     src\u001b[38;5;241m=\u001b[39mmji, index\u001b[38;5;241m=\u001b[39mreceiver, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes\n\u001b[1;32m    756\u001b[0m )  \u001b[38;5;66;03m# [n_nodes, irreps]\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/e3nn/nn/_fc.py:43\u001b[0m, in \u001b[0;36m_Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_in \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_in)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m w\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m(x)\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_out\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs, weights = trainer.fit(model, solver_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13758857",
   "metadata": {},
   "source": [
    "Run the model to get **predictions** on the whole validation set. We record simple metrics such as energy and force MAE.\n",
    "\n",
    "> NOTE: The **forces_mode** parameter can be set to either \"torch.func\" or \"torch.autograd\". The two have different performance characteristics: \"torch.func\" is more suitable when there are many different models being trained (i.e. many different solver hyperparameters) as it can effectively run these in a batch way. When only a few models are present, \"torch.autograd\" can be much faster. In our testing the cutoff between the two is approximately at 100 models, but this may vary greatly depending on hardware characteristics.\n",
    "\n",
    "> NOTE: predictions and metric tracking can also be done with the `trainer.evaluate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_mae = franken.metrics.init_metric(\"energy_MAE\", device=device)\n",
    "forces_mae = franken.metrics.init_metric(\"forces_MAE\", device=device)\n",
    "franken_predictions = []\n",
    "for atom_data, targets in tqdm(val_dl, desc=\"Predicting energies and forces\"):\n",
    "    # Move the data to the GPU\n",
    "    atom_data = atom_data.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "    pred_e, pred_f = model.energy_and_forces(\n",
    "        atom_data,\n",
    "        weights=weights,\n",
    "        forces_mode=\"torch.autograd\",\n",
    "        add_energy_shift=True\n",
    "    )\n",
    "    predictions = Target(pred_e, pred_f)\n",
    "    energy_mae.update(predictions, targets)\n",
    "    forces_mae.update(predictions, targets)\n",
    "    franken_predictions.append(predictions.to(device=\"cpu\"))\n",
    "energy_mae = energy_mae.compute()\n",
    "forces_mae = forces_mae.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90897499",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Energy error = {energy_mae} eV/atom\")\n",
    "print(f\"Forces error = {forces_mae} eV/Angstrom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aab9fc",
   "metadata": {},
   "source": [
    "### Compare against zero-shot MACE\n",
    "\n",
    "To show the magnitude of the improvement in accuracy, we evaluate the corresponding MACE\n",
    "model zero-shot (i.e. without fine-tuning) on the same water dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04928415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mace.calculators import mace_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ce2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the MACE-L0 backbone, same as before.\n",
    "# The interaction block is not relevant for this experiment.\n",
    "gnn_config = MaceBackboneConfig(\n",
    "    path_or_id=\"MACE-L0\",\n",
    "    interaction_block=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b429620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using float32 for MACECalculator, which is faster but less accurate. Recommended for MD. Use float64 for geometry optimization.\n",
      "Default dtype float32 does not match model dtype float64, converting models to float32.\n"
     ]
    }
   ],
   "source": [
    "from franken.backbones.utils import get_checkpoint_path\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# get_checkpoint_path will download the checkpoint if it doesn't exist in the franken cache.\n",
    "macemp0_path = get_checkpoint_path(gnn_config.path_or_id)\n",
    "zeroshot_macemp = mace_mp(macemp0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data, we will use the ase Atoms object directly\n",
    "# since `zeroshot_macemp` is an ase Calculator object.\n",
    "ase_val_dset = ase.io.read(\n",
    "    DATASET_REGISTRY.get_path(\"water\", \"val\", base_path=CacheDir.get()),\n",
    "    index=\":\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c19db2",
   "metadata": {},
   "source": [
    "Model and dataset have been loaded, we only need to run the evaluation.\n",
    "\n",
    "Note that since the model is not calibrated to the potential energy of this system, the ground-truth and predicted energies will have a constant difference\n",
    "which influences error calculations.\n",
    "To fix this we compute the error up to the constant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302238f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fffa8469164a389276729839d8cbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting energies and forces:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zeroshot_predictions = []\n",
    "for atom_data in tqdm(ase_val_dset, desc=\"Predicting energies and forces\"):\n",
    "    zeroshot_macemp.calculate(atoms=atom_data)\n",
    "    predictions = Target(\n",
    "        torch.tensor(zeroshot_macemp.results[\"energy\"]),\n",
    "        torch.from_numpy(zeroshot_macemp.results[\"forces\"])\n",
    "    )\n",
    "    zeroshot_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we find the constant correction for the predictions\n",
    "all_pred_energy = []\n",
    "all_target_energy = []\n",
    "for i, atom_data in enumerate(ase_val_dset):\n",
    "    targets = Target(\n",
    "        energy=torch.tensor(atom_data.get_potential_energy()).float(),\n",
    "        forces=torch.from_numpy(atom_data.get_forces()).float(),\n",
    "    )\n",
    "    predictions = zeroshot_predictions[i]\n",
    "    all_target_energy.append(targets.energy)\n",
    "    all_pred_energy.append(predictions.energy)\n",
    "constant_difference = torch.tensor(all_pred_energy).mean() - torch.tensor(all_target_energy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e45728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally compute the metrics with corrected free energies\n",
    "zeroshot_energy_mae = franken.metrics.init_metric(\"energy_MAE\", device=\"cpu\")\n",
    "zeroshot_forces_mae = franken.metrics.init_metric(\"forces_MAE\", device=\"cpu\")\n",
    "for i, atom_data in enumerate(ase_val_dset):\n",
    "    targets = Target(\n",
    "        energy=torch.tensor(atom_data.get_potential_energy()).float(),\n",
    "        forces=torch.from_numpy(atom_data.get_forces()).float(),\n",
    "    )\n",
    "    zeroshot_predictions[i] = Target(\n",
    "        energy=zeroshot_predictions[i].energy - constant_difference,\n",
    "        forces=zeroshot_predictions[i].forces\n",
    "    )\n",
    "    zeroshot_energy_mae.update(zeroshot_predictions[i], targets)\n",
    "    zeroshot_forces_mae.update(zeroshot_predictions[i], targets)\n",
    "zeroshot_energy_mae = zeroshot_energy_mae.compute()\n",
    "zeroshot_forces_mae = zeroshot_forces_mae.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccb3455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot MACE-MP0 energy error = tensor([2.3443]) eV/atom\n",
      "Zero-shot MACE-MP0 forces error = tensor([132.8901]) eV/Angstrom\n"
     ]
    }
   ],
   "source": [
    "print(f\"Zero-shot MACE-MP0 energy error = {zeroshot_energy_mae} eV/atom\")\n",
    "print(f\"Zero-shot MACE-MP0 forces error = {zeroshot_forces_mae} eV/Angstrom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66f0e9",
   "metadata": {},
   "source": [
    "We can also plot the error distribution for both methods to see the large improvement with `franken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51555c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_mae = franken.metrics.init_metric(\"energy_MAE\", device=\"cpu\")\n",
    "forces_mae = franken.metrics.init_metric(\"forces_MAE\", device=\"cpu\")\n",
    "errors = {\n",
    "    \"energy\": {\n",
    "        \"franken\": [],\n",
    "        \"zeroshot\": []\n",
    "    },\n",
    "    \"forces\": {\n",
    "        \"franken\": [],\n",
    "        \"zeroshot\": []\n",
    "    }\n",
    "}\n",
    "for i, atom_data in enumerate(ase_val_dset):\n",
    "    targets = Target(\n",
    "        energy=torch.tensor(atom_data.get_potential_energy()).float(),\n",
    "        forces=torch.from_numpy(atom_data.get_forces()).float(),\n",
    "    )\n",
    "    energy_mae.update(zeroshot_predictions[i], targets)\n",
    "    errors[\"energy\"][\"zeroshot\"].append(energy_mae.compute(reset=True).item())\n",
    "    energy_mae.update(franken_predictions[i], targets)\n",
    "    errors[\"energy\"][\"franken\"].append(energy_mae.compute(reset=True).item())\n",
    "    forces_mae.update(zeroshot_predictions[i], targets)\n",
    "    errors[\"forces\"][\"zeroshot\"].append(forces_mae.compute(reset=True).item())\n",
    "    forces_mae.update(franken_predictions[i], targets)\n",
    "    errors[\"forces\"][\"franken\"].append(forces_mae.compute(reset=True).item())\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "ax[0].violinplot([errors[\"energy\"][\"zeroshot\"], errors[\"energy\"][\"franken\"]], positions=[0, 1], showmedians=True)\n",
    "ax[0].set_xticks([0, 1], [\"Zero shot\", \"Franken\"])\n",
    "ax[0].set_ylabel(\"Energy MAE (eV/atom)\")\n",
    "ax[0].set_title(\"Energy errors\")\n",
    "ax[1].violinplot([errors[\"forces\"][\"zeroshot\"], errors[\"forces\"][\"franken\"]], positions=[0, 1], showmedians=True)\n",
    "ax[1].set_xticks([0, 1], [\"Zero shot\", \"Franken\"])\n",
    "ax[1].set_ylabel(\"Forces MAE (eV/)\")\n",
    "ax[1].set_title(\"Force errors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
